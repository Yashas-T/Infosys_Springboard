{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_HKgmI7MeYz"
      },
      "source": [
        "##Section 1:- Setup and Installations\n",
        "\n",
        "This initial section prepares the environment for the entire benchmarking process. It runs a pip install command to download and install all required Python libraries, such as transformers, torch, accelerate, and radon. After the installations are complete, it imports all necessary modules, making them available for the subsequent sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c-SE7yzbMgAS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Setup Complete! \n"
          ]
        }
      ],
      "source": [
        "# --- Section 1: Setup & Installations ---\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install transformers torch accelerate bitsandbytes pandas huggingface_hub radon ipywidgets matplotlib seaborn -q\n",
        "\n",
        "# Import necessary modules\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "import ast\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from radon.complexity import cc_visit\n",
        "from radon.metrics import mi_visit\n",
        "from radon.raw import analyze\n",
        "\n",
        "print(\"\\nSetup Complete! \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MBBmFWmMrlk"
      },
      "source": [
        "##Section 2 :- Configuration & Backend Engine\n",
        "##1)GEMMA\n",
        "This cell defines the core logic of the application. It's split into two main parts:\n",
        "Model Configuration: A dictionary named MODELS_TO_TEST is defined here, allowing you to easily specify which models from the Hugging Face Hub you want to benchmark. It also sets the computation device to \"cuda\" if a GPU is available, or \"cpu\" otherwise.\n",
        "\n",
        "Helper & Generation Functions: This part contains several crucial functions:\n",
        "\n",
        "clean_generated_code(): Removes model-specific formatting and extracts the pure Python code from the raw output.\n",
        "\n",
        "is_syntactically_valid(): Uses Python's ast module to check if the generated code is free of syntax errors.\n",
        "\n",
        "calculate_advanced_metrics(): Calculates Cyclomatic Complexity, Maintainability Index, and Lines of Code for syntactically valid code.\n",
        "\n",
        "generate_code(): The main function that formats the prompt, sends it to the model, times the generation process, and returns the cleaned code and generation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DIaApsTrMuvx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend engine with advanced metrics is ready.\n"
          ]
        }
      ],
      "source": [
        "# --- Model Configuration ---\n",
        "MODELS_TO_TEST = {\n",
        "    #\"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    #\"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    }\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Helper & Generation Functions ---\n",
        "def clean_generated_code(text, model_path):\n",
        "    model_path = model_path.lower()\n",
        "    if \"gemma\" in model_path: text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
        "    elif \"phi-2\" in model_path: text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
        "    else: text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
        "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
        "    if match: text = match.group(1)\n",
        "    return text.strip()\n",
        "\n",
        "def is_syntactically_valid(code_string: str) -> bool:\n",
        "    if not code_string: return False\n",
        "    try: ast.parse(code_string); return True\n",
        "    except SyntaxError: return False\n",
        "\n",
        "def calculate_advanced_metrics(code_string):\n",
        "    if not is_syntactically_valid(code_string):\n",
        "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "    try:\n",
        "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
        "        maintainability = mi_visit(code_string, multi=True)\n",
        "        loc = analyze(code_string).loc\n",
        "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
        "    except: return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    model_path = model.name_or_path.lower()\n",
        "    if \"gemma\" in model_path: formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    elif \"phi-2\" in model_path: formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
        "    else: formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
        "    start_time = time.time()\n",
        "    output_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "    end_time = time.time()\n",
        "\n",
        "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
        "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
        "\n",
        "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
        "\n",
        "print(\"Backend engine with advanced metrics is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCKrV4VVMxzi"
      },
      "source": [
        "##Section 3 :-  Pre-Loading All AI Models\n",
        "To ensure that the benchmarking process is efficient and measures only the code generation time, this section pre-loads all models specified in the MODELS_TO_TEST dictionary into memory. It iterates through each model, downloads its weights and tokenizer from the Hugging Face Hub, and stores them in a loaded_models dictionary for quick access later. This step can be time-consuming depending on the size of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iW8nRRb2Mygv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to pre-load all models...\n",
            "\n",
            "--- Loading Gemma-2B-IT... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3e8f6465cbf466d8bb96d84af2adbb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "612201b9cd004a19a042e1a930069d21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42296f358fc043f1a95a3c89cede3db5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6b8de0a697b4817a5d27aafb5be2c32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd521b5dc1ae48f8a26bfc89fbef596c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c2bd8c85a4841c5b46892d1469663c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36f012ca216b43c5a5a1dd8ec9939ca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e793e6af7de4432b180520f38f603fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9c1ce070fcb47dfa94f445013a54e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d7523ef31844bbc9fcd13df2edda379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dacfdff19bb44a3a29b5aa8aa374285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma-2B-IT loaded successfully.\n",
            "\n",
            "==================================================\n",
            "All available models are pre-loaded.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Section 3: Pre-Loading All Models ---\n",
        "loaded_models = {}\n",
        "print(\"Starting to pre-load all models...\")\n",
        "for model_name, model_path in MODELS_TO_TEST.items():\n",
        "    print(f\"\\n--- Loading {model_name}... ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
        "        loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "        print(f\"{model_name} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED to load {model_name}. Error: {e}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePdcApTM7Qh"
      },
      "source": [
        "##Section 4 :- UI #1 - Benchmark All Models\n",
        "This section introduces the first interactive user interface for basic benchmarking. It consists of:\n",
        "\n",
        "A Textarea for entering a single coding prompt.\n",
        "\n",
        "A Run Benchmark button to start the process.\n",
        "\n",
        "An Output area to display the results.\n",
        "\n",
        "When the button is clicked, the notebook runs the prompt on every pre-loaded model and displays a pandas DataFrame comparing their generated code, generation time, and all calculated code quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FiIQNc5SM8gE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UI #1: Benchmark All Models ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Textarea(value='', layout=Layout(width='95%'), placeholder='Enter a prompt to benchmark all mod…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a237850fe2d8491093bfd2466452b50b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# --- Section 4: UI #1 - Run All Models ---\n",
        "print(\"--- UI #1: Benchmark All Models ---\")\n",
        "all_results_log = [] # Global log to store all results from both UIs\n",
        "\n",
        "prompt_input_all = widgets.Textarea(placeholder='Enter a prompt to benchmark all models...', layout={'width': '95%'})\n",
        "run_all_button = widgets.Button(description='Run Benchmark', button_style='danger', icon='rocket')\n",
        "output_all = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
        "\n",
        "def on_run_all_clicked(b):\n",
        "    with output_all:\n",
        "        prompt = prompt_input_all.value\n",
        "        if not prompt: print(\"Please enter a prompt.\"); return\n",
        "\n",
        "        print(f\"Running prompt on {len(loaded_models)} models...\")\n",
        "        results_this_run = []\n",
        "        for model_name, components in loaded_models.items():\n",
        "            print(f\"  - Generating with {model_name}...\")\n",
        "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "            metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "            results_this_run.append(entry)\n",
        "            all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Benchmark Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "run_all_button.on_click(on_run_all_clicked)\n",
        "display(widgets.VBox([prompt_input_all, run_all_button, output_all]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzWg9AYNArA"
      },
      "source": [
        "## Section 5 :-  UI #2 - Inspect Models with Checkboxes #\n",
        "This is a more advanced UI designed for flexible and multi-prompt testing. Its features include:\n",
        "\n",
        "A larger Textarea that accepts up to 10 different prompts, each on a new line.\n",
        "\n",
        "A series of Checkbox widgets, one for each loaded model, allowing you to select which ones to include in the run.\n",
        "\n",
        "A Run Selected button that triggers the benchmark for every selected prompt-model combination.\n",
        "\n",
        "This UI is ideal for more targeted testing or for evaluating how different models perform on a variety of tasks in a single run.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\")\n",
        "\n",
        "# Text area for multiple prompts\n",
        "prompt_input_selected = widgets.Textarea(\n",
        "    placeholder='Enter up to 10 prompts, each on a new line...',\n",
        "    layout={'width': '95%', 'height': '150px'}\n",
        ")\n",
        "\n",
        "run_selected_button = widgets.Button(\n",
        "    description='Run Selected',\n",
        "    button_style='success',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "output_selected = widgets.Output(\n",
        "    layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'}\n",
        ")\n",
        "\n",
        "# Checkboxes for selecting models\n",
        "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
        "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
        "\n",
        "# Button click event\n",
        "def on_run_selected_clicked(b):\n",
        "    with output_selected:\n",
        "        output_selected.clear_output(wait=True)\n",
        "        prompts_raw = prompt_input_selected.value.strip()\n",
        "        if not prompts_raw:\n",
        "            print(\"Please enter one or more prompts.\")\n",
        "            return\n",
        "\n",
        "        # Split by line and take up to 10 prompts\n",
        "        prompts = [p.strip() for p in prompts_raw.splitlines() if p.strip()]\n",
        "        if not prompts:\n",
        "            print(\"No valid prompts found.\")\n",
        "            return\n",
        "        if len(prompts) > 10:\n",
        "            print(\"Warning: Only the first 10 prompts will be used.\")\n",
        "            prompts = prompts[:10]\n",
        "\n",
        "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
        "        if not models_to_run:\n",
        "            print(\"Please select at least one model.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Running {len(prompts)} prompts on {len(models_to_run)} selected models...\\n\")\n",
        "\n",
        "        results_this_run = []\n",
        "\n",
        "        # Run each prompt through each model\n",
        "        for idx, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nPrompt {idx}: {prompt}\\n\")\n",
        "            for model_name in models_to_run:\n",
        "                print(f\"  - Generating with {model_name}...\")\n",
        "                components = loaded_models[model_name]\n",
        "                result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "                metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "                entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "                results_this_run.append(entry)\n",
        "                all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Multi-Prompt Run Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "# Attach event handler\n",
        "run_selected_button.on_click(on_run_selected_clicked)\n",
        "\n",
        "# Build and display UI\n",
        "ui_selected_models = widgets.VBox([\n",
        "    widgets.HTML(\"<h4>Enter up to 10 prompts (one per line):</h4>\"),\n",
        "    prompt_input_selected,\n",
        "    widgets.HTML(\"<h4>Select models to run:</h4>\"),\n",
        "    checkbox_container,\n",
        "    run_selected_button,\n",
        "    output_selected\n",
        "])\n",
        "\n",
        "display(ui_selected_models)\n"
      ],
      "metadata": {
        "id": "bWtfAePW4qwP"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h4>Enter up to 10 prompts (one per line):</h4>'), Textarea(value='', layout=Layout…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5b5b0ce006c45a4b91aab92337f8fc0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qihTUeBRNEY-"
      },
      "source": [
        "##Section 6 :- Final Analysis and Visualization Report\n",
        "After conducting one or more benchmark runs, this section allows you to generate a comprehensive report. Clicking the Generate Full Report & Plots button does two things:\n",
        "\n",
        "It displays a complete pandas DataFrame containing all results logged during the session from both UIs.\n",
        "\n",
        "It generates three comparative bar charts using matplotlib and seaborn to visually compare the models based on Generation Time, Cyclomatic Complexity, and Maintainability Index, making it easy to see which models perform best across these key metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NAratfJPNFOR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Use the button below to generate the final report for the session.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Button(button_style='info', description='Generate Full Report & Plots', style=ButtonStyle()), O…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "235768b4a73e40caab2c6ee4b49fefbd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "report_button = widgets.Button(description=\"Generate Full Report & Plots\", button_style='info')\n",
        "report_output = widgets.Output()\n",
        "\n",
        "def on_report_button_clicked(b):\n",
        "    with report_output:\n",
        "        report_output.clear_output(wait=True)\n",
        "        if not all_results_log:\n",
        "            print(\"No results logged. Use one of the UIs above to generate code.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results_log).round(2)\n",
        "        df.rename(columns={'complexity': 'Complexity', 'maintainability': 'Maintainability', 'gen_time': 'Gen Time (s)'}, inplace=True)\n",
        "\n",
        "        print(\"--- Full Session Data ---\")\n",
        "        display(df)\n",
        "\n",
        "        print(\"\\n--- Comparative Plots ---\")\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        plot_df = df.dropna(subset=['Complexity', 'Maintainability'])\n",
        "\n",
        "        if plot_df.empty:\n",
        "            print(\"Not enough valid data to generate plots.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "        fig.suptitle('Comparative Analysis of Code Metrics', fontsize=16)\n",
        "\n",
        "        sns.barplot(ax=axes[0], data=plot_df, x='Model', y='Gen Time (s)', palette='viridis')\n",
        "        axes[0].set_title('Generation Time (Lower is Faster)')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[1], data=plot_df, x='Model', y='Complexity', palette='magma')\n",
        "        axes[1].set_title('Cyclomatic Complexity (Lower is Simpler)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[2], data=plot_df, x='Model', y='Maintainability', palette='plasma')\n",
        "        axes[2].set_title('Maintainability Index (Higher is Better)')\n",
        "        axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "report_button.on_click(on_report_button_clicked)\n",
        "print(\"\\nUse the button below to generate the final report for the session.\")\n",
        "display(widgets.VBox([report_button, report_output]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdvLi2JTFYT"
      },
      "source": [
        "## Section  7:- (Optional) Manual Cleanup\n",
        "\n",
        "This final, optional cell provides a utility function to free up system resources. When run, it deletes all the loaded models and tokenizers from memory and then calls torch.cuda.empty_cache() to clear the GPU's VRAM. This is useful for avoiding memory errors if you plan to continue using the notebook for other tasks without restarting the kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gYoV1Y6XTGKD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing 1 models from memory...\n",
            "\n",
            "All models have been cleared from GPU memory.\n"
          ]
        }
      ],
      "source": [
        "def clear_all_models():\n",
        "    global loaded_models\n",
        "    print(f\"Clearing {len(loaded_models)} models from memory...\")\n",
        "    for model_name in list(loaded_models.keys()):\n",
        "        del loaded_models[model_name]['model']\n",
        "        del loaded_models[model_name]['tokenizer']\n",
        "        del loaded_models[model_name]\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\nAll models have been cleared from GPU memory.\")\n",
        "\n",
        "# To run the cleanup, uncomment and run the line below:\n",
        "clear_all_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PART-B"
      ],
      "metadata": {
        "id": "nsfpRktIsH1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2 :- Configuration & Backend Engine\n",
        "##2)DEEPSEEK\n",
        "This cell defines the core logic of the application. It's split into two main parts:\n",
        "Model Configuration: A dictionary named MODELS_TO_TEST is defined here, allowing you to easily specify which models from the Hugging Face Hub you want to benchmark. It also sets the computation device to \"cuda\" if a GPU is available, or \"cpu\" otherwise.\n",
        "\n",
        "Helper & Generation Functions: This part contains several crucial functions:\n",
        "\n",
        "clean_generated_code(): Removes model-specific formatting and extracts the pure Python code from the raw output.\n",
        "\n",
        "is_syntactically_valid(): Uses Python's ast module to check if the generated code is free of syntax errors.\n",
        "\n",
        "calculate_advanced_metrics(): Calculates Cyclomatic Complexity, Maintainability Index, and Lines of Code for syntactically valid code.\n",
        "\n",
        "generate_code(): The main function that formats the prompt, sends it to the model, times the generation process, and returns the cleaned code and generation time.\n"
      ],
      "metadata": {
        "id": "YJEaOjEahUSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i46f2dsMhTdQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend engine with advanced metrics is ready.\n"
          ]
        }
      ],
      "source": [
        "# --- Model Configuration ---\n",
        "MODELS_TO_TEST = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "    #\"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    #\"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    }\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Helper & Generation Functions ---\n",
        "def clean_generated_code(text, model_path):\n",
        "    model_path = model_path.lower()\n",
        "    if \"gemma\" in model_path: text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
        "    elif \"phi-2\" in model_path: text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
        "    else: text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
        "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
        "    if match: text = match.group(1)\n",
        "    return text.strip()\n",
        "\n",
        "def is_syntactically_valid(code_string: str) -> bool:\n",
        "    if not code_string: return False\n",
        "    try: ast.parse(code_string); return True\n",
        "    except SyntaxError: return False\n",
        "\n",
        "def calculate_advanced_metrics(code_string):\n",
        "    if not is_syntactically_valid(code_string):\n",
        "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "    try:\n",
        "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
        "        maintainability = mi_visit(code_string, multi=True)\n",
        "        loc = analyze(code_string).loc\n",
        "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
        "    except: return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    model_path = model.name_or_path.lower()\n",
        "    if \"gemma\" in model_path: formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    elif \"phi-2\" in model_path: formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
        "    else: formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
        "    start_time = time.time()\n",
        "    output_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "    end_time = time.time()\n",
        "\n",
        "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
        "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
        "\n",
        "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
        "\n",
        "print(\"Backend engine with advanced metrics is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 3: Pre-Loading All AI Models\n",
        "To ensure that the benchmarking process is efficient and measures only the code generation time, this section pre-loads all models specified in the MODELS_TO_TEST dictionary into memory. It iterates through each model, downloads its weights and tokenizer from the Hugging Face Hub, and stores them in a loaded_models dictionary for quick access later. This step can be time-consuming depending on the size of the models."
      ],
      "metadata": {
        "id": "kdPPA0gGtU2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Pre-Loading All Models ---\n",
        "loaded_models = {}\n",
        "print(\"Starting to pre-load all models...\")\n",
        "for model_name, model_path in MODELS_TO_TEST.items():\n",
        "    print(f\"\\n--- Loading {model_name}... ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
        "        loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "        print(f\"{model_name} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED to load {model_name}. Error: {e}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "l3c9libEhc8E"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to pre-load all models...\n",
            "\n",
            "--- Loading DeepSeek-Coder-1.3B... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5274e23a7b144837a0d6310255a3d82a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "245ce48a39c24019be4a4f3a969e948c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad4b0329a0114712b07567b015b58d7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e7e1d1e75b84825a5844d8f9a3985ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d21ce9779d6143129273fbd7c464a839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek-Coder-1.3B loaded successfully.\n",
            "\n",
            "==================================================\n",
            "All available models are pre-loaded.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 4: UI #1 - Benchmark All Models\n",
        "This section introduces the first interactive user interface for basic benchmarking. It consists of:\n",
        "\n",
        "A Textarea for entering a single coding prompt.\n",
        "\n",
        "A Run Benchmark button to start the process.\n",
        "\n",
        "An Output area to display the results.\n",
        "\n",
        "When the button is clicked, the notebook runs the prompt on every pre-loaded model and displays a pandas DataFrame comparing their generated code, generation time, and all calculated code quality metrics."
      ],
      "metadata": {
        "id": "nozDbCPgte-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: UI #1 - Run All Models ---\n",
        "print(\"--- UI #1: Benchmark All Models ---\")\n",
        "all_results_log = [] # Global log to store all results from both UIs\n",
        "\n",
        "prompt_input_all = widgets.Textarea(placeholder='Enter a prompt to benchmark all models...', layout={'width': '95%'})\n",
        "run_all_button = widgets.Button(description='Run Benchmark', button_style='danger', icon='rocket')\n",
        "output_all = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
        "\n",
        "def on_run_all_clicked(b):\n",
        "    with output_all:\n",
        "        prompt = prompt_input_all.value\n",
        "        if not prompt: print(\"Please enter a prompt.\"); return\n",
        "\n",
        "        print(f\"Running prompt on {len(loaded_models)} models...\")\n",
        "        results_this_run = []\n",
        "        for model_name, components in loaded_models.items():\n",
        "            print(f\"  - Generating with {model_name}...\")\n",
        "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "            metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "            results_this_run.append(entry)\n",
        "            all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Benchmark Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "run_all_button.on_click(on_run_all_clicked)\n",
        "display(widgets.VBox([prompt_input_all, run_all_button, output_all]))"
      ],
      "metadata": {
        "id": "ageLUO72hhet"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UI #1: Benchmark All Models ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Textarea(value='', layout=Layout(width='95%'), placeholder='Enter a prompt to benchmark all mod…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "124c4bbd975f4e1892c15a372672ce7d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 5: UI #2 - Inspect Models with Checkboxes\n",
        "This is a more advanced UI designed for flexible and multi-prompt testing. Its features include:\n",
        "\n",
        "A larger Textarea that accepts up to 10 different prompts, each on a new line.\n",
        "\n",
        "A series of Checkbox widgets, one for each loaded model, allowing you to select which ones to include in the run.\n",
        "\n",
        "A Run Selected button that triggers the benchmark for every selected prompt-model combination.\n",
        "\n",
        "This UI is ideal for more targeted testing or for evaluating how different models perform on a variety of tasks in a single run."
      ],
      "metadata": {
        "id": "BGQmLUANtmct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\")\n",
        "\n",
        "# Text area for multiple prompts\n",
        "prompt_input_selected = widgets.Textarea(\n",
        "    placeholder='Enter up to 10 prompts, each on a new line...',\n",
        "    layout={'width': '95%', 'height': '150px'}\n",
        ")\n",
        "\n",
        "run_selected_button = widgets.Button(\n",
        "    description='Run Selected',\n",
        "    button_style='success',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "output_selected = widgets.Output(\n",
        "    layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'}\n",
        ")\n",
        "\n",
        "# Checkboxes for selecting models\n",
        "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
        "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
        "\n",
        "# Button click event\n",
        "def on_run_selected_clicked(b):\n",
        "    with output_selected:\n",
        "        output_selected.clear_output(wait=True)\n",
        "        prompts_raw = prompt_input_selected.value.strip()\n",
        "        if not prompts_raw:\n",
        "            print(\"Please enter one or more prompts.\")\n",
        "            return\n",
        "\n",
        "        # Split by line and take up to 10 prompts\n",
        "        prompts = [p.strip() for p in prompts_raw.splitlines() if p.strip()]\n",
        "        if not prompts:\n",
        "            print(\"No valid prompts found.\")\n",
        "            return\n",
        "        if len(prompts) > 10:\n",
        "            print(\"Warning: Only the first 10 prompts will be used.\")\n",
        "            prompts = prompts[:10]\n",
        "\n",
        "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
        "        if not models_to_run:\n",
        "            print(\"Please select at least one model.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Running {len(prompts)} prompts on {len(models_to_run)} selected models...\\n\")\n",
        "\n",
        "        results_this_run = []\n",
        "\n",
        "        # Run each prompt through each model\n",
        "        for idx, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nPrompt {idx}: {prompt}\\n\")\n",
        "            for model_name in models_to_run:\n",
        "                print(f\"  - Generating with {model_name}...\")\n",
        "                components = loaded_models[model_name]\n",
        "                result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "                metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "                entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "                results_this_run.append(entry)\n",
        "                all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Multi-Prompt Run Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "# Attach event handler\n",
        "run_selected_button.on_click(on_run_selected_clicked)\n",
        "\n",
        "# Build and display UI\n",
        "ui_selected_models = widgets.VBox([\n",
        "    widgets.HTML(\"<h4>Enter up to 10 prompts (one per line):</h4>\"),\n",
        "    prompt_input_selected,\n",
        "    widgets.HTML(\"<h4>Select models to run:</h4>\"),\n",
        "    checkbox_container,\n",
        "    run_selected_button,\n",
        "    output_selected\n",
        "])\n",
        "\n",
        "display(ui_selected_models)\n"
      ],
      "metadata": {
        "id": "buQ_9yyyho1I"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h4>Enter up to 10 prompts (one per line):</h4>'), Textarea(value='', layout=Layout…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aab365776c6e4674b5a8742e5a62ca04"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 6: Final Analysis and Visualization Report\n",
        "After conducting one or more benchmark runs, this section allows you to generate a comprehensive report. Clicking the Generate Full Report & Plots button does two things:\n",
        "\n",
        "It displays a complete pandas DataFrame containing all results logged during the session from both UIs.\n",
        "\n",
        "It generates three comparative bar charts using matplotlib and seaborn to visually compare the models based on Generation Time, Cyclomatic Complexity, and Maintainability Index, making it easy to see which models perform best across these key metrics."
      ],
      "metadata": {
        "id": "XiGwCg9juDQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_button = widgets.Button(description=\"Generate Full Report & Plots\", button_style='info')\n",
        "report_output = widgets.Output()\n",
        "\n",
        "def on_report_button_clicked(b):\n",
        "    with report_output:\n",
        "        report_output.clear_output(wait=True)\n",
        "        if not all_results_log:\n",
        "            print(\"No results logged. Use one of the UIs above to generate code.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results_log).round(2)\n",
        "        df.rename(columns={'complexity': 'Complexity', 'maintainability': 'Maintainability', 'gen_time': 'Gen Time (s)'}, inplace=True)\n",
        "\n",
        "        print(\"--- Full Session Data ---\")\n",
        "        display(df)\n",
        "\n",
        "        print(\"\\n--- Comparative Plots ---\")\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        plot_df = df.dropna(subset=['Complexity', 'Maintainability'])\n",
        "\n",
        "        if plot_df.empty:\n",
        "            print(\"Not enough valid data to generate plots.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "        fig.suptitle('Comparative Analysis of Code Metrics', fontsize=16)\n",
        "\n",
        "        sns.barplot(ax=axes[0], data=plot_df, x='Model', y='Gen Time (s)', palette='viridis')\n",
        "        axes[0].set_title('Generation Time (Lower is Faster)')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[1], data=plot_df, x='Model', y='Complexity', palette='magma')\n",
        "        axes[1].set_title('Cyclomatic Complexity (Lower is Simpler)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[2], data=plot_df, x='Model', y='Maintainability', palette='plasma')\n",
        "        axes[2].set_title('Maintainability Index (Higher is Better)')\n",
        "        axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "report_button.on_click(on_report_button_clicked)\n",
        "print(\"\\nUse the button below to generate the final report for the session.\")\n",
        "display(widgets.VBox([report_button, report_output]))"
      ],
      "metadata": {
        "id": "j5WIlNJChqan"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Use the button below to generate the final report for the session.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Button(button_style='info', description='Generate Full Report & Plots', style=ButtonStyle()), O…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c06dcf74a31247e482641305244d7a1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 7: Manual Cleanup (Optional)\n",
        "This final, optional cell provides a utility function to free up system resources. When run, it deletes all the loaded models and tokenizers from memory and then calls torch.cuda.empty_cache() to clear the GPU's VRAM. This is useful for avoiding memory errors if you plan to continue using the notebook for other tasks without restarting the kernel."
      ],
      "metadata": {
        "id": "QFZb5rCkuPW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_all_models():\n",
        "    global loaded_models\n",
        "    print(f\"Clearing {len(loaded_models)} models from memory...\")\n",
        "    for model_name in list(loaded_models.keys()):\n",
        "        del loaded_models[model_name]['model']\n",
        "        del loaded_models[model_name]['tokenizer']\n",
        "        del loaded_models[model_name]\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\nAll models have been cleared from GPU memory.\")\n",
        "\n",
        "# To run the cleanup, uncomment and run the line below:\n",
        "clear_all_models()"
      ],
      "metadata": {
        "id": "wLoiISBchzc0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing 1 models from memory...\n",
            "\n",
            "All models have been cleared from GPU memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PART-C"
      ],
      "metadata": {
        "id": "aYFPe0iNh0ER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2 :- Configuration & Backend Engine\n",
        "##3)PHI-2\n",
        "This cell defines the core logic of the application. It's split into two main parts:\n",
        "Model Configuration: A dictionary named MODELS_TO_TEST is defined here, allowing you to easily specify which models from the Hugging Face Hub you want to benchmark. It also sets the computation device to \"cuda\" if a GPU is available, or \"cpu\" otherwise.\n",
        "\n",
        "Helper & Generation Functions: This part contains several crucial functions:\n",
        "\n",
        "clean_generated_code(): Removes model-specific formatting and extracts the pure Python code from the raw output.\n",
        "\n",
        "is_syntactically_valid(): Uses Python's ast module to check if the generated code is free of syntax errors.\n",
        "\n",
        "calculate_advanced_metrics(): Calculates Cyclomatic Complexity, Maintainability Index, and Lines of Code for syntactically valid code.\n",
        "\n",
        "generate_code(): The main function that formats the prompt, sends it to the model, times the generation process, and returns the cleaned code and generation time.\n"
      ],
      "metadata": {
        "id": "tZzxa9RMulNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Configuration ---\n",
        "MODELS_TO_TEST = {\n",
        "    #\"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\"\n",
        "    #\"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    }\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Helper & Generation Functions ---\n",
        "def clean_generated_code(text, model_path):\n",
        "    model_path = model_path.lower()\n",
        "    if \"gemma\" in model_path: text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
        "    elif \"phi-2\" in model_path: text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
        "    else: text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
        "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
        "    if match: text = match.group(1)\n",
        "    return text.strip()\n",
        "\n",
        "def is_syntactically_valid(code_string: str) -> bool:\n",
        "    if not code_string: return False\n",
        "    try: ast.parse(code_string); return True\n",
        "    except SyntaxError: return False\n",
        "\n",
        "def calculate_advanced_metrics(code_string):\n",
        "    if not is_syntactically_valid(code_string):\n",
        "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "    try:\n",
        "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
        "        maintainability = mi_visit(code_string, multi=True)\n",
        "        loc = analyze(code_string).loc\n",
        "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
        "    except: return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    model_path = model.name_or_path.lower()\n",
        "    if \"gemma\" in model_path: formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    elif \"phi-2\" in model_path: formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
        "    else: formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
        "    start_time = time.time()\n",
        "    output_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "    end_time = time.time()\n",
        "\n",
        "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
        "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
        "\n",
        "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
        "\n",
        "print(\"Backend engine with advanced metrics is ready.\")"
      ],
      "metadata": {
        "id": "cvkYMEuzh0dq"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend engine with advanced metrics is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 3: Pre-Loading All AI Models\n",
        "To ensure that the benchmarking process is efficient and measures only the code generation time, this section pre-loads all models specified in the MODELS_TO_TEST dictionary into memory. It iterates through each model, downloads its weights and tokenizer from the Hugging Face Hub, and stores them in a loaded_models dictionary for quick access later. This step can be time-consuming depending on the size of the models."
      ],
      "metadata": {
        "id": "6d_s1FELvODM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Pre-Loading All Models ---\n",
        "loaded_models = {}\n",
        "print(\"Starting to pre-load all models...\")\n",
        "for model_name, model_path in MODELS_TO_TEST.items():\n",
        "    print(f\"\\n--- Loading {model_name}... ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
        "        loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "        print(f\"{model_name} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED to load {model_name}. Error: {e}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "p1orcTGriKA5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to pre-load all models...\n",
            "\n",
            "--- Loading Phi-2-2.7B... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11fb194e10bb49fdbd686a72f99718d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "678362e920af4bc9876db1b7ec8eeeda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b5ac82cf92a4ef9b0666af12bb6b679"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c4d04692126462ea4cee2046bffd4ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb9ea7f9331f4475ad66470f8e47b443"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7ea59dda5004f4990c6db5061be8db3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d95c774d1fd4418892e1aa65a86c4257"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ee8bd99f5634465a234cc74094b4441"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bf19748496b413f9bba1542e9749636"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df8648d34cdc4f13b62a94431562dffb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5bd52707fb544bea45aeef8eefa03a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5cce00696ce4d74aa07d9fa2a4b63a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a78c2474e4b74e8dadb3ef43fe3b4dbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi-2-2.7B loaded successfully.\n",
            "\n",
            "==================================================\n",
            "All available models are pre-loaded.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 4: UI #1 - Benchmark All Models\n",
        "\n",
        "This section introduces the first interactive user interface for basic benchmarking. It consists of:\n",
        "\n",
        "A Textarea for entering a single coding prompt.\n",
        "\n",
        "A Run Benchmark button to start the process.\n",
        "\n",
        "An Output area to display the results.\n",
        "\n",
        "When the button is clicked, the notebook runs the prompt on every pre-loaded model and displays a pandas DataFrame comparing their generated code, generation time, and all calculated code quality metrics."
      ],
      "metadata": {
        "id": "zpKZ1ueGvVNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: UI #1 - Run All Models ---\n",
        "print(\"--- UI #1: Benchmark All Models ---\")\n",
        "all_results_log = [] # Global log to store all results from both UIs\n",
        "\n",
        "prompt_input_all = widgets.Textarea(placeholder='Enter a prompt to benchmark all models...', layout={'width': '95%'})\n",
        "run_all_button = widgets.Button(description='Run Benchmark', button_style='danger', icon='rocket')\n",
        "output_all = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
        "\n",
        "def on_run_all_clicked(b):\n",
        "    with output_all:\n",
        "        prompt = prompt_input_all.value\n",
        "        if not prompt: print(\"Please enter a prompt.\"); return\n",
        "\n",
        "        print(f\"Running prompt on {len(loaded_models)} models...\")\n",
        "        results_this_run = []\n",
        "        for model_name, components in loaded_models.items():\n",
        "            print(f\"  - Generating with {model_name}...\")\n",
        "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "            metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "            results_this_run.append(entry)\n",
        "            all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Benchmark Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "run_all_button.on_click(on_run_all_clicked)\n",
        "display(widgets.VBox([prompt_input_all, run_all_button, output_all]))"
      ],
      "metadata": {
        "id": "XUGyPppJiOg8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UI #1: Benchmark All Models ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Textarea(value='', layout=Layout(width='95%'), placeholder='Enter a prompt to benchmark all mod…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2611569e4f343d1b26e1d1353cbfef2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 5: UI #2 - Inspect Models with Checkboxes\n",
        "This is a more advanced UI designed for flexible and multi-prompt testing. Its features include:\n",
        "\n",
        "A larger Textarea that accepts up to 10 different prompts, each on a new line.\n",
        "\n",
        "A series of Checkbox widgets, one for each loaded model, allowing you to select which ones to include in the run.\n",
        "\n",
        "A Run Selected button that triggers the benchmark for every selected prompt-model combination.\n",
        "\n",
        "This UI is ideal for more targeted testing or for evaluating how different models perform on a variety of tasks in a single run."
      ],
      "metadata": {
        "id": "KsdgcQqGvfXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\")\n",
        "\n",
        "# Text area for multiple prompts\n",
        "prompt_input_selected = widgets.Textarea(\n",
        "    placeholder='Enter up to 10 prompts, each on a new line...',\n",
        "    layout={'width': '95%', 'height': '150px'}\n",
        ")\n",
        "\n",
        "run_selected_button = widgets.Button(\n",
        "    description='Run Selected',\n",
        "    button_style='success',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "output_selected = widgets.Output(\n",
        "    layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'}\n",
        ")\n",
        "\n",
        "# Checkboxes for selecting models\n",
        "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
        "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
        "\n",
        "# Button click event\n",
        "def on_run_selected_clicked(b):\n",
        "    with output_selected:\n",
        "        output_selected.clear_output(wait=True)\n",
        "        prompts_raw = prompt_input_selected.value.strip()\n",
        "        if not prompts_raw:\n",
        "            print(\"Please enter one or more prompts.\")\n",
        "            return\n",
        "\n",
        "        # Split by line and take up to 10 prompts\n",
        "        prompts = [p.strip() for p in prompts_raw.splitlines() if p.strip()]\n",
        "        if not prompts:\n",
        "            print(\"No valid prompts found.\")\n",
        "            return\n",
        "        if len(prompts) > 10:\n",
        "            print(\"Warning: Only the first 10 prompts will be used.\")\n",
        "            prompts = prompts[:10]\n",
        "\n",
        "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
        "        if not models_to_run:\n",
        "            print(\"Please select at least one model.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Running {len(prompts)} prompts on {len(models_to_run)} selected models...\\n\")\n",
        "\n",
        "        results_this_run = []\n",
        "\n",
        "        # Run each prompt through each model\n",
        "        for idx, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nPrompt {idx}: {prompt}\\n\")\n",
        "            for model_name in models_to_run:\n",
        "                print(f\"  - Generating with {model_name}...\")\n",
        "                components = loaded_models[model_name]\n",
        "                result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "                metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "                entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "                results_this_run.append(entry)\n",
        "                all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Multi-Prompt Run Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "# Attach event handler\n",
        "run_selected_button.on_click(on_run_selected_clicked)\n",
        "\n",
        "# Build and display UI\n",
        "ui_selected_models = widgets.VBox([\n",
        "    widgets.HTML(\"<h4>Enter up to 10 prompts (one per line):</h4>\"),\n",
        "    prompt_input_selected,\n",
        "    widgets.HTML(\"<h4>Select models to run:</h4>\"),\n",
        "    checkbox_container,\n",
        "    run_selected_button,\n",
        "    output_selected\n",
        "])\n",
        "\n",
        "display(ui_selected_models)\n"
      ],
      "metadata": {
        "id": "aDUMGOCNiQR-"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h4>Enter up to 10 prompts (one per line):</h4>'), Textarea(value='', layout=Layout…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99c80df1f98d4b7aae7c97913832e68d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 6: Final Analysis and Visualization Report\n",
        "After conducting one or more benchmark runs, this section allows you to generate a comprehensive report. Clicking the Generate Full Report & Plots button does two things:\n",
        "\n",
        "It displays a complete pandas DataFrame containing all results logged during the session from both UIs.\n",
        "\n",
        "It generates three comparative bar charts using matplotlib and seaborn to visually compare the models based on Generation Time, Cyclomatic Complexity, and Maintainability Index, making it easy to see which models perform best across these key metrics."
      ],
      "metadata": {
        "id": "xcAOzaUYvlGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_button = widgets.Button(description=\"Generate Full Report & Plots\", button_style='info')\n",
        "report_output = widgets.Output()\n",
        "\n",
        "def on_report_button_clicked(b):\n",
        "    with report_output:\n",
        "        report_output.clear_output(wait=True)\n",
        "        if not all_results_log:\n",
        "            print(\"No results logged. Use one of the UIs above to generate code.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results_log).round(2)\n",
        "        df.rename(columns={'complexity': 'Complexity', 'maintainability': 'Maintainability', 'gen_time': 'Gen Time (s)'}, inplace=True)\n",
        "\n",
        "        print(\"--- Full Session Data ---\")\n",
        "        display(df)\n",
        "\n",
        "        print(\"\\n--- Comparative Plots ---\")\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        plot_df = df.dropna(subset=['Complexity', 'Maintainability'])\n",
        "\n",
        "        if plot_df.empty:\n",
        "            print(\"Not enough valid data to generate plots.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "        fig.suptitle('Comparative Analysis of Code Metrics', fontsize=16)\n",
        "\n",
        "        sns.barplot(ax=axes[0], data=plot_df, x='Model', y='Gen Time (s)', palette='viridis')\n",
        "        axes[0].set_title('Generation Time (Lower is Faster)')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[1], data=plot_df, x='Model', y='Complexity', palette='magma')\n",
        "        axes[1].set_title('Cyclomatic Complexity (Lower is Simpler)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[2], data=plot_df, x='Model', y='Maintainability', palette='plasma')\n",
        "        axes[2].set_title('Maintainability Index (Higher is Better)')\n",
        "        axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "report_button.on_click(on_report_button_clicked)\n",
        "print(\"\\nUse the button below to generate the final report for the session.\")\n",
        "display(widgets.VBox([report_button, report_output]))"
      ],
      "metadata": {
        "id": "Q_zUNjZ8iTXS"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Use the button below to generate the final report for the session.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Button(button_style='info', description='Generate Full Report & Plots', style=ButtonStyle()), O…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eae01c78888f48beb7070293ed0e450c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 7: Manual Cleanup (Optional)\n",
        "This final, optional cell provides a utility function to free up system resources. When run, it deletes all the loaded models and tokenizers from memory and then calls torch.cuda.empty_cache() to clear the GPU's VRAM. This is useful for avoiding memory errors if you plan to continue using the notebook for other tasks without restarting the kernel."
      ],
      "metadata": {
        "id": "d_LYU-N2vrVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clear_all_models():\n",
        "    global loaded_models\n",
        "    print(f\"Clearing {len(loaded_models)} models from memory...\")\n",
        "    for model_name in list(loaded_models.keys()):\n",
        "        del loaded_models[model_name]['model']\n",
        "        del loaded_models[model_name]['tokenizer']\n",
        "        del loaded_models[model_name]\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\nAll models have been cleared from GPU memory.\")\n",
        "\n",
        "# To run the cleanup, uncomment and run the line below:\n",
        "clear_all_models()"
      ],
      "metadata": {
        "id": "KAnx2ciRiViL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}