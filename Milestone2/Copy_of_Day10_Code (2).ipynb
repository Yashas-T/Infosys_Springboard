{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installations\n",
        "Explanation: This first section prepares the environment. It installs all the necessary libraries for the project, including ipywidgets for the user interface, transformers for the AI models, radon for code metrics, and seaborn for plotting.\n",
        "\n"
      ],
      "metadata": {
        "id": "-_HKgmI7MeYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 1: Setup & Installations ---\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install transformers torch accelerate bitsandbytes pandas huggingface_hub radon ipywidgets matplotlib seaborn -q\n",
        "\n",
        "# Import necessary modules\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "import ast\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from radon.complexity import cc_visit\n",
        "from radon.metrics import mi_visit\n",
        "from radon.raw import analyze\n",
        "\n",
        "print(\"\\nSetup Complete! ✅\")"
      ],
      "metadata": {
        "id": "c-SE7yzbMgAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Secure Hugging Face Login\n",
        "Explanation: This section handles authentication, which is required for any \"gated\" models you may use. Run this cell and enter your Hugging Face access token when prompted.\n",
        "\n"
      ],
      "metadata": {
        "id": "sMyNf_uYMqzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration & Backend Engine\n",
        "Explanation: This section defines the models to test and all the backend helper functions, including the updated calculate_advanced_metrics function which now computes Halstead metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "7MBBmFWmMrlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Configuration & Backend Engine ---\n",
        "# --- Model Configuration ---\n",
        "MODELS_TO_TEST = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Helper & Generation Functions ---\n",
        "def clean_generated_code(text, model_path):\n",
        "    model_path = model_path.lower()\n",
        "    if \"gemma\" in model_path: text = re.sub(r\"<start_of_turn>user\\n.*<end_of_turn>\\n<start_of_turn>model\\n\", \"\", text, flags=re.DOTALL).replace(\"<end_of_turn>\", \"\")\n",
        "    elif \"phi-2\" in model_path: text = re.sub(r\"Instruct:.*\\nOutput:\", \"\", text, flags=re.DOTALL)\n",
        "    else: text = re.sub(r\"### Instruction:\\n.*\\n\\n### Response:\", \"\", text, flags=re.DOTALL)\n",
        "    match = re.search(r\"```python\\n(.*?)\\n```\", text, re.DOTALL)\n",
        "    if match: text = match.group(1)\n",
        "    return text.strip()\n",
        "\n",
        "def is_syntactically_valid(code_string: str) -> bool:\n",
        "    if not code_string: return False\n",
        "    try: ast.parse(code_string); return True\n",
        "    except SyntaxError: return False\n",
        "\n",
        "def calculate_advanced_metrics(code_string):\n",
        "    if not is_syntactically_valid(code_string):\n",
        "        return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "    try:\n",
        "        complexity = sum([c.complexity for c in cc_visit(code_string)]) if cc_visit(code_string) else 0\n",
        "        maintainability = mi_visit(code_string, multi=True)\n",
        "        loc = analyze(code_string).loc\n",
        "        return {\"complexity\": complexity, \"maintainability\": round(float(maintainability), 2), \"loc\": loc}\n",
        "    except: return {\"complexity\": None, \"maintainability\": None, \"loc\": None}\n",
        "\n",
        "def generate_code(model, tokenizer, prompt):\n",
        "    model_path = model.name_or_path.lower()\n",
        "    if \"gemma\" in model_path: formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    elif \"phi-2\" in model_path: formatted_prompt = f\"Instruct: {prompt}\\nOutput:\"\n",
        "    else: formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
        "    start_time = time.time()\n",
        "    output_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "    end_time = time.time()\n",
        "\n",
        "    raw_output = tokenizer.batch_decode(output_ids)[0]\n",
        "    cleaned_code = clean_generated_code(raw_output, model_path)\n",
        "\n",
        "    return {\"code\": cleaned_code, \"gen_time\": end_time - start_time}\n",
        "\n",
        "print(\"Backend engine with advanced metrics is ready.\")"
      ],
      "metadata": {
        "id": "DIaApsTrMuvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pre-Loading All AI Models\n",
        "Explanation: This section pre-loads all models into memory. This will take several minutes but will result in a much faster UI experience. Warning: This will use a significant amount of your GPU memory."
      ],
      "metadata": {
        "id": "rCKrV4VVMxzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Pre-Loading All Models ---\n",
        "loaded_models = {}\n",
        "print(\"Starting to pre-load all models...\")\n",
        "for model_name, model_path in MODELS_TO_TEST.items():\n",
        "    print(f\"\\n--- Loading {model_name}... ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
        "        loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "        print(f\"✅ {model_name} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED to load {model_name}. Error: {e}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll available models are pre-loaded.\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "iW8nRRb2Mygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. UI #1: Benchmark All Models\n",
        "Explanation: This first interface is for broad, comparative benchmarking. Enter a single prompt, and it will run against every pre-loaded model, displaying the results and metrics in a table."
      ],
      "metadata": {
        "id": "gePdcApTM7Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 5: UI #1 - Run All Models ---\n",
        "print(\"--- UI #1: Benchmark All Models ---\")\n",
        "all_results_log = [] # Global log to store all results from both UIs\n",
        "\n",
        "prompt_input_all = widgets.Textarea(placeholder='Enter a prompt to benchmark all models...', layout={'width': '95%'})\n",
        "run_all_button = widgets.Button(description='Run Benchmark', button_style='danger', icon='rocket')\n",
        "output_all = widgets.Output(layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'})\n",
        "\n",
        "def on_run_all_clicked(b):\n",
        "    with output_all:\n",
        "        prompt = prompt_input_all.value\n",
        "        if not prompt: print(\"Please enter a prompt.\"); return\n",
        "\n",
        "        print(f\"Running prompt on {len(loaded_models)} models...\")\n",
        "        results_this_run = []\n",
        "        for model_name, components in loaded_models.items():\n",
        "            print(f\"  - Generating with {model_name}...\")\n",
        "            result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "            metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "            entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "            results_this_run.append(entry)\n",
        "            all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Benchmark Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "run_all_button.on_click(on_run_all_clicked)\n",
        "display(widgets.VBox([prompt_input_all, run_all_button, output_all]))"
      ],
      "metadata": {
        "id": "FiIQNc5SM8gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. UI #2: Inspect Models with Checkboxes\n",
        "Explanation: This second interface provides more control. It creates a checkbox for each model, allowing you to run a prompt on a specific subset of the pre-loaded models.\n"
      ],
      "metadata": {
        "id": "oJzWg9AYNArA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- UI #2: Inspect Selected Models (Multi-Prompt Version) ---\")\n",
        "\n",
        "# Text area for multiple prompts\n",
        "prompt_input_selected = widgets.Textarea(\n",
        "    placeholder='Enter up to 10 prompts, each on a new line...',\n",
        "    layout={'width': '95%', 'height': '150px'}\n",
        ")\n",
        "\n",
        "run_selected_button = widgets.Button(\n",
        "    description='Run Selected',\n",
        "    button_style='success',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "output_selected = widgets.Output(\n",
        "    layout={'border': '1px solid black', 'padding': '10px', 'overflow': 'scroll'}\n",
        ")\n",
        "\n",
        "# Checkboxes for selecting models\n",
        "model_checkboxes = {name: widgets.Checkbox(value=True, description=name) for name in loaded_models.keys()}\n",
        "checkbox_container = widgets.VBox(list(model_checkboxes.values()))\n",
        "\n",
        "# Button click event\n",
        "def on_run_selected_clicked(b):\n",
        "    with output_selected:\n",
        "        output_selected.clear_output(wait=True)\n",
        "        prompts_raw = prompt_input_selected.value.strip()\n",
        "        if not prompts_raw:\n",
        "            print(\"Please enter one or more prompts.\")\n",
        "            return\n",
        "\n",
        "        # Split by line and take up to 10 prompts\n",
        "        prompts = [p.strip() for p in prompts_raw.splitlines() if p.strip()]\n",
        "        if not prompts:\n",
        "            print(\"No valid prompts found.\")\n",
        "            return\n",
        "        if len(prompts) > 10:\n",
        "            print(\"Warning: Only the first 10 prompts will be used.\")\n",
        "            prompts = prompts[:10]\n",
        "\n",
        "        models_to_run = [name for name, cb in model_checkboxes.items() if cb.value]\n",
        "        if not models_to_run:\n",
        "            print(\"Please select at least one model.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Running {len(prompts)} prompts on {len(models_to_run)} selected models...\\n\")\n",
        "\n",
        "        results_this_run = []\n",
        "\n",
        "        # Run each prompt through each model\n",
        "        for idx, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nPrompt {idx}: {prompt}\\n\")\n",
        "            for model_name in models_to_run:\n",
        "                print(f\"  - Generating with {model_name}...\")\n",
        "                components = loaded_models[model_name]\n",
        "                result = generate_code(components['model'], components['tokenizer'], prompt)\n",
        "                metrics = calculate_advanced_metrics(result['code'])\n",
        "\n",
        "                entry = {'Model': model_name, 'Prompt': prompt, **result, **metrics}\n",
        "                results_this_run.append(entry)\n",
        "                all_results_log.append(entry)\n",
        "\n",
        "        print(\"\\n--- Multi-Prompt Run Complete ---\")\n",
        "        results_df = pd.DataFrame(results_this_run).round(2)\n",
        "        display(HTML(results_df.to_html().replace('\\\\n', '<br>')))\n",
        "\n",
        "# Attach event handler\n",
        "run_selected_button.on_click(on_run_selected_clicked)\n",
        "\n",
        "# Build and display UI\n",
        "ui_selected_models = widgets.VBox([\n",
        "    widgets.HTML(\"<h4>Enter up to 10 prompts (one per line):</h4>\"),\n",
        "    prompt_input_selected,\n",
        "    widgets.HTML(\"<h4>Select models to run:</h4>\"),\n",
        "    checkbox_container,\n",
        "    run_selected_button,\n",
        "    output_selected\n",
        "])\n",
        "\n",
        "display(ui_selected_models)\n"
      ],
      "metadata": {
        "id": "mZbOxESNDBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Final Analysis and Visualization Report\n",
        "Explanation: After using either UI to generate results, run this section. A button will appear that generates a comprehensive report, including a full data table and comparative plots for the key metrics across all tests run in your session."
      ],
      "metadata": {
        "id": "qihTUeBRNEY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 7: Final Analysis and Visualization Report ---\n",
        "report_button = widgets.Button(description=\"Generate Full Report & Plots\", button_style='info')\n",
        "report_output = widgets.Output()\n",
        "\n",
        "def on_report_button_clicked(b):\n",
        "    with report_output:\n",
        "        report_output.clear_output(wait=True)\n",
        "        if not all_results_log:\n",
        "            print(\"No results logged. Use one of the UIs above to generate code.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results_log).round(2)\n",
        "        df.rename(columns={'complexity': 'Complexity', 'maintainability': 'Maintainability', 'gen_time': 'Gen Time (s)'}, inplace=True)\n",
        "\n",
        "        print(\"--- Full Session Data ---\")\n",
        "        display(df)\n",
        "\n",
        "        print(\"\\n--- Comparative Plots ---\")\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        plot_df = df.dropna(subset=['Complexity', 'Maintainability'])\n",
        "\n",
        "        if plot_df.empty:\n",
        "            print(\"Not enough valid data to generate plots.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "        fig.suptitle('Comparative Analysis of Code Metrics', fontsize=16)\n",
        "\n",
        "        sns.barplot(ax=axes[0], data=plot_df, x='Model', y='Gen Time (s)', palette='viridis')\n",
        "        axes[0].set_title('Generation Time (Lower is Faster)')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[1], data=plot_df, x='Model', y='Complexity', palette='magma')\n",
        "        axes[1].set_title('Cyclomatic Complexity (Lower is Simpler)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        sns.barplot(ax=axes[2], data=plot_df, x='Model', y='Maintainability', palette='plasma')\n",
        "        axes[2].set_title('Maintainability Index (Higher is Better)')\n",
        "        axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "report_button.on_click(on_report_button_clicked)\n",
        "print(\"\\nUse the button below to generate the final report for the session.\")\n",
        "display(widgets.VBox([report_button, report_output]))"
      ],
      "metadata": {
        "id": "NAratfJPNFOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. (Optional) Manual Cleanup\n",
        "Explanation: Run this cell to manually clear all pre-loaded models from memory and free up your GPU resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "oKdvLi2JTFYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 8: Optional Manual Cleanup ---\n",
        "def clear_all_models():\n",
        "    global loaded_models\n",
        "    print(f\"Clearing {len(loaded_models)} models from memory...\")\n",
        "    for model_name in list(loaded_models.keys()):\n",
        "        del loaded_models[model_name]['model']\n",
        "        del loaded_models[model_name]['tokenizer']\n",
        "        del loaded_models[model_name]\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\n✅ All models have been cleared from GPU memory.\")\n",
        "\n",
        "# To run the cleanup, uncomment and run the line below:\n",
        "clear_all_models()"
      ],
      "metadata": {
        "id": "gYoV1Y6XTGKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}